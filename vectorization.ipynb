{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenize_nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6f2a47233183>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize_nltk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenize_nltk' is not defined"
     ]
    }
   ],
   "source": [
    "tokens = tokenize_nltk(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-3e07cede08f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokens' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = nltk.tokenize.TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4850"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "len(stopwords.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlpkf.tokenizer import TokenizerNltk, TokenizerSpacy, Tokenizer\n",
    "from nlpkf.vectorizers import OneHotGensim, create_word_vectorizer, WordToVec\n",
    "from nlpkf.utils import text_to_sentence_vecs, text_to_nchunk_vecs, text_to_word_vecs, text_to_vector, clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"The elephant sneezed at the sight of potatoes. \",\n",
    "    \"Bats can see via echolocation. See the bat sight sneeze!\",\n",
    "    \"Wondering, she opened the door to the studio.\",\n",
    "]\n",
    "text = clean_text(\"\".join(corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 31, 31)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = text_to_word_vecs(text)\n",
    "len(x), len([i[:3] for i in x]), len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([-0.52547556,  1.8666574 ,  0.13036793], dtype=float32),\n",
       "  array([-0.57379955,  0.50064933,  2.5647202 ], dtype=float32)),\n",
       " (array([-0.57379955,  0.50064933,  2.5647202 ], dtype=float32),\n",
       "  array([-3.8646727, -2.0901725, -2.6483977], dtype=float32)),\n",
       " (array([-3.8646727, -2.0901725, -2.6483977], dtype=float32),\n",
       "  array([ 3.3111951,  0.5545187, -0.6642161], dtype=float32)),\n",
       " (array([ 3.3111951,  0.5545187, -0.6642161], dtype=float32),\n",
       "  array([0.25327787, 0.87766397, 2.5326617 ], dtype=float32)),\n",
       " (array([0.25327787, 0.87766397, 2.5326617 ], dtype=float32),\n",
       "  array([ 0.7013794,  2.420463 , -0.8726411], dtype=float32)),\n",
       " (array([ 0.7013794,  2.420463 , -0.8726411], dtype=float32),\n",
       "  array([ 2.7941532,  2.0729694, -1.4409533], dtype=float32)),\n",
       " (array([ 2.7941532,  2.0729694, -1.4409533], dtype=float32),\n",
       "  array([-0.06581615,  2.4987152 ,  3.5946229 ], dtype=float32)),\n",
       " (array([-0.06581615,  2.4987152 ,  3.5946229 ], dtype=float32),\n",
       "  array([0.38723868, 2.8492007 , 0.63496786], dtype=float32)),\n",
       " (array([0.38723868, 2.8492007 , 0.63496786], dtype=float32),\n",
       "  array([-1.8141156,  5.4153633,  3.4820998], dtype=float32)),\n",
       " (array([-1.8141156,  5.4153633,  3.4820998], dtype=float32),\n",
       "  array([-2.7639976, -1.3185803, -2.48119  ], dtype=float32)),\n",
       " (array([-2.7639976, -1.3185803, -2.48119  ], dtype=float32),\n",
       "  array([0.42997754, 1.3741974 , 1.3904492 ], dtype=float32)),\n",
       " (array([0.42997754, 1.3741974 , 1.3904492 ], dtype=float32),\n",
       "  array([ 2.4353104 ,  0.51944506, -1.9094467 ], dtype=float32)),\n",
       " (array([ 2.4353104 ,  0.51944506, -1.9094467 ], dtype=float32),\n",
       "  array([2.513407  , 0.57301325, 3.1085455 ], dtype=float32)),\n",
       " (array([2.513407  , 0.57301325, 3.1085455 ], dtype=float32),\n",
       "  array([ 3.0011148 , -0.86371344,  1.2114279 ], dtype=float32)),\n",
       " (array([ 3.0011148 , -0.86371344,  1.2114279 ], dtype=float32),\n",
       "  array([ 0.66619503,  2.5494843 , -0.09488085], dtype=float32)),\n",
       " (array([ 0.66619503,  2.5494843 , -0.09488085], dtype=float32),\n",
       "  array([ 2.2382848 , -0.48754793,  0.7722964 ], dtype=float32)),\n",
       " (array([ 2.2382848 , -0.48754793,  0.7722964 ], dtype=float32),\n",
       "  array([-0.4175677, -1.295501 ,  1.172246 ], dtype=float32)),\n",
       " (array([-0.4175677, -1.295501 ,  1.172246 ], dtype=float32),\n",
       "  array([-0.6792848 ,  2.8369968 , -0.44031873], dtype=float32)),\n",
       " (array([-0.6792848 ,  2.8369968 , -0.44031873], dtype=float32),\n",
       "  array([0.64567274, 2.2592096 , 0.9582199 ], dtype=float32)),\n",
       " (array([0.64567274, 2.2592096 , 0.9582199 ], dtype=float32),\n",
       "  array([ 3.3942828 ,  1.001253  , -0.42683017], dtype=float32)),\n",
       " (array([ 3.3942828 ,  1.001253  , -0.42683017], dtype=float32),\n",
       "  array([-0.9172003,  1.1787841, -1.2058625], dtype=float32)),\n",
       " (array([-0.9172003,  1.1787841, -1.2058625], dtype=float32),\n",
       "  array([ 1.6989292 , -0.31841308,  0.07794186], dtype=float32)),\n",
       " (array([ 1.6989292 , -0.31841308,  0.07794186], dtype=float32),\n",
       "  array([-1.564398  , -0.76023364,  7.3710423 ], dtype=float32)),\n",
       " (array([-1.564398  , -0.76023364,  7.3710423 ], dtype=float32),\n",
       "  array([-3.314066  , -0.19269753, -2.887737  ], dtype=float32)),\n",
       " (array([-3.314066  , -0.19269753, -2.887737  ], dtype=float32),\n",
       "  array([ 2.8908012, -0.919958 ,  1.6292256], dtype=float32)),\n",
       " (array([ 2.8908012, -0.919958 ,  1.6292256], dtype=float32),\n",
       "  array([2.8240373 , 0.23243427, 1.4116691 ], dtype=float32)),\n",
       " (array([2.8240373 , 0.23243427, 1.4116691 ], dtype=float32),\n",
       "  array([ 2.66236   ,  1.687345  , -0.06195392], dtype=float32)),\n",
       " (array([ 2.66236   ,  1.687345  , -0.06195392], dtype=float32),\n",
       "  array([1.849877 , 1.0047926, 3.8482194], dtype=float32)),\n",
       " (array([1.849877 , 1.0047926, 3.8482194], dtype=float32),\n",
       "  array([-0.02689242, -0.42950723,  2.41211   ], dtype=float32)),\n",
       " (array([-0.02689242, -0.42950723,  2.41211   ], dtype=float32),\n",
       "  array([ 2.8707075, -1.5407771,  2.1335237], dtype=float32))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "list(nltk.ngrams([i[:3] for i in x], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'elephant'),\n",
       " ('elephant', 'sneezed'),\n",
       " ('sneezed', 'at'),\n",
       " ('at', 'the'),\n",
       " ('the', 'sight'),\n",
       " ('sight', 'of'),\n",
       " ('of', 'potatoes'),\n",
       " ('potatoes', '.'),\n",
       " ('.', 'Bats'),\n",
       " ('Bats', 'can'),\n",
       " ('can', 'see'),\n",
       " ('see', 'via'),\n",
       " ('via', 'echolocation'),\n",
       " ('echolocation', '.'),\n",
       " ('.', 'See'),\n",
       " ('See', 'the'),\n",
       " ('the', 'bat'),\n",
       " ('bat', 'sight'),\n",
       " ('sight', 'sneeze'),\n",
       " ('sneeze', '!'),\n",
       " ('!', 'Wondering'),\n",
       " ('Wondering', ','),\n",
       " (',', 'she'),\n",
       " ('she', 'opened'),\n",
       " ('opened', 'the'),\n",
       " ('the', 'door'),\n",
       " ('door', 'to'),\n",
       " ('to', 'the'),\n",
       " ('the', 'studio'),\n",
       " ('studio', '.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.ngrams(text.split(), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, array([-5.25475562e-01,  1.86665738e+00,  1.30367935e-01,  4.99081898e+00,\n",
       "        -3.38221788e-02,  1.00722432e+00,  9.34666634e-01,  3.70950490e-01,\n",
       "         1.35799193e+00,  2.89854097e+00,  9.60064352e-01,  1.47703195e+00,\n",
       "         6.94060326e-01, -2.92380714e+00, -9.49871242e-01, -3.35054064e+00,\n",
       "         3.01920557e+00,  3.23980808e-01, -5.87301612e-01,  1.42875504e+00,\n",
       "         4.22025156e+00,  2.83829999e+00, -1.58517635e+00,  1.47718504e-01,\n",
       "         4.02612734e+00, -1.98015428e+00,  8.86760712e-01,  9.33763206e-01,\n",
       "        -1.88373482e+00, -2.32407165e+00,  6.99570403e-03,  4.35480499e+00,\n",
       "         9.29226160e-01, -3.75017017e-01,  3.80339682e-01, -1.51936579e+00,\n",
       "         5.54521751e+00, -2.50310397e+00, -1.89730871e+00, -4.35285687e-01,\n",
       "        -2.05240512e+00, -9.75709498e-01,  1.27878642e+00, -1.76949251e+00,\n",
       "        -9.02045369e-01, -3.08017468e+00,  2.67038822e+00,  3.18578267e+00,\n",
       "        -1.22527051e+00,  5.41229963e-01,  3.42318177e-01, -1.01836467e+00,\n",
       "        -1.93205762e+00, -3.01184011e+00, -1.49560976e+00,  4.42146015e+00,\n",
       "         6.90020025e-01,  1.04811764e+00,  1.41383469e-01,  1.08104908e+00,\n",
       "         2.81818837e-01,  1.86882889e+00,  1.42269945e+00,  1.51710081e+00,\n",
       "        -1.70830739e+00, -3.57842708e+00,  1.65683270e-01,  1.30952883e+00,\n",
       "         3.81072235e+00,  1.01226628e+00,  1.06140125e+00, -2.38617778e+00,\n",
       "        -1.96923053e+00,  2.91476178e+00,  5.93719840e-01, -5.56093359e+00,\n",
       "        -5.21896929e-02, -3.40956116e+00, -1.11684406e+00,  9.43830609e-02,\n",
       "         1.69541049e+00, -1.02296543e+00, -2.00352097e+00, -1.08605361e+00,\n",
       "        -1.38681555e+00, -2.18588638e+00, -7.81872869e-01,  2.62056637e+00,\n",
       "         3.70791018e-01,  3.87791967e+00,  1.00202298e+00,  3.12186384e+00,\n",
       "         3.56607842e+00,  3.13643718e+00,  1.40498781e+00, -2.70860887e+00,\n",
       "        -1.31883264e+00, -4.30296063e-01, -1.31855726e+00, -3.35249233e+00,\n",
       "         7.48878658e-01, -2.59486675e+00,  8.12074304e-01, -1.59243941e+00,\n",
       "         1.17369913e-01,  1.49753857e+00, -8.33386540e-01, -4.28657627e+00,\n",
       "         9.92797852e-01,  1.53280759e+00, -8.60023379e-01, -2.86955976e+00,\n",
       "        -2.77807951e+00,  2.08279562e+00,  8.39997768e-01,  1.53558159e+00,\n",
       "        -2.12953162e+00, -2.98520303e+00, -1.05193055e+00, -1.08934999e+00,\n",
       "        -1.70739639e+00,  2.03667116e+00,  1.36491370e+00, -2.53368068e+00,\n",
       "        -1.38298392e-01, -1.26432562e+00, -1.72857428e+00, -2.90514135e+00,\n",
       "         6.66778684e-02, -6.01168275e-01,  2.20917046e-01, -5.58066428e-01,\n",
       "        -9.33572352e-02, -2.41840690e-01, -6.89882278e-01, -7.81896114e-02,\n",
       "         1.04152560e+00,  7.12650359e-01, -1.44109890e-01, -5.48099935e-01,\n",
       "         1.65179121e+00, -2.84388423e-01, -5.91365218e-01,  9.09454167e-01,\n",
       "         2.42751211e-01,  2.11976409e-01,  2.96796978e-01, -7.64583498e-02,\n",
       "        -3.24330032e-02,  1.06840479e+00, -4.05730665e-01, -2.81851411e-01,\n",
       "        -1.07718706e-02, -7.70628750e-02, -3.74546528e-01,  5.34817576e-02,\n",
       "         6.81581259e-01, -7.73570776e-01,  1.36019319e-01,  7.19214559e-01,\n",
       "        -7.27742910e-01, -4.27695900e-01, -1.14433259e-01, -2.38163501e-01,\n",
       "        -3.12510937e-01,  3.98279041e-01,  2.92815685e-01,  1.02882028e+00,\n",
       "        -6.03642225e-01,  3.52958024e-01, -8.60573053e-01,  1.04292107e+00,\n",
       "         3.85725737e-01,  6.99000239e-01,  1.15720594e+00, -1.32524908e-01,\n",
       "        -1.66721806e-01, -8.72434855e-01, -4.56656337e-01,  1.72720850e-03,\n",
       "        -7.18634367e-01,  3.16440344e-01, -2.81008095e-01, -3.44420224e-01,\n",
       "         1.86750472e-01,  2.30010778e-01,  3.86289716e-01,  7.24032044e-01,\n",
       "        -5.78975320e-01, -6.61685646e-01, -6.76555634e-01,  1.85956717e+00,\n",
       "        -6.75615072e-02, -2.80030668e-02, -4.03252766e-02, -4.06804323e-01,\n",
       "         4.45615977e-01,  3.20998222e-01, -1.15657210e-01, -3.70429754e-02,\n",
       "         6.13066912e-01,  5.11385143e-01,  3.27592790e-01,  1.90816760e-01,\n",
       "         8.58059227e-02, -4.93083060e-01,  2.06018269e-01, -9.63320136e-02,\n",
       "        -2.03662217e-02, -3.90396863e-01, -7.26311922e-01, -4.78792995e-01,\n",
       "         4.49179411e-02,  2.04063654e-01, -4.89977449e-01, -3.85805130e-01,\n",
       "        -1.25290465e+00,  4.15630937e-02,  2.81799138e-01, -5.81707954e-01,\n",
       "        -3.30876291e-01, -1.02625275e+00, -7.19964057e-02,  2.93030232e-01,\n",
       "         2.91437089e-01, -3.79570216e-01, -6.63850665e-01,  5.96043319e-02,\n",
       "        -1.51535034e-01, -1.43107176e-02,  2.99280167e-01,  3.90332878e-01,\n",
       "         9.81740594e-01,  3.75397146e-01, -2.16851890e-01, -3.14781398e-01,\n",
       "         3.27183485e-01,  5.87269187e-01,  8.65465522e-01, -1.27427876e-02,\n",
       "        -3.62309873e-01, -3.91768605e-01, -6.92983091e-01, -1.03207850e+00,\n",
       "         1.01349103e+00, -2.95090467e-01, -1.54730737e-01, -9.49261338e-02,\n",
       "        -1.64442122e-01,  4.72358823e-01, -1.41890824e-01, -5.12594461e-01,\n",
       "         4.02985811e-01, -4.68529612e-01, -3.13500375e-01, -3.46375048e-01,\n",
       "        -2.36539945e-01,  6.54423535e-02,  2.66649425e-01, -2.27795333e-01,\n",
       "        -3.26859087e-01, -1.20928437e-01, -3.63872677e-01,  2.48682380e-01,\n",
       "         4.52341676e-01, -2.59030819e-01, -1.89251989e-01, -1.34167045e-01,\n",
       "        -2.09671959e-01,  5.49088717e-01, -2.51051068e-01,  6.36591554e-01,\n",
       "         4.53007072e-01,  1.54536381e-01, -4.37127233e-01, -3.73969257e-01,\n",
       "         3.61066461e-01, -4.81071204e-01,  7.01692343e-01,  1.72132656e-01,\n",
       "        -5.36232352e-01,  6.09962702e-01, -1.30574450e-01,  2.19391853e-01,\n",
       "         6.70434952e-01, -5.31744137e-02,  1.36727944e-01, -1.95806086e-01,\n",
       "        -4.05443698e-01,  2.32635438e-03,  7.97262341e-02,  5.48335761e-02,\n",
       "         1.45078301e-01,  2.35192627e-01,  2.47053593e-01, -7.44722337e-02,\n",
       "        -4.78075862e-01,  7.41224408e-01, -3.54438335e-01,  1.71481445e-02,\n",
       "         2.39727169e-01,  1.99614123e-01,  8.49551916e-01, -4.10861671e-01,\n",
       "         1.27837807e-01,  1.09199516e-01,  1.03099859e+00, -9.97272611e-01,\n",
       "        -1.27059817e-01,  4.38703775e-01,  2.68224478e-01, -1.09267384e-02,\n",
       "        -3.53975520e-02, -1.97480336e-01,  5.83105147e-01, -5.00063181e-01,\n",
       "         1.81105942e-01,  1.73853755e-01, -2.68162191e-01, -3.24852049e-01,\n",
       "        -8.02226067e-02,  6.35551751e-01, -6.25657797e-01,  9.16529298e-01,\n",
       "        -4.49166521e-02, -3.87240171e-01,  4.15172130e-02,  9.90242213e-02,\n",
       "         3.14726621e-01,  4.27460074e-02,  1.91282287e-01,  1.55662894e-01,\n",
       "        -4.42611128e-01,  9.44465756e-01,  3.41519117e-02, -9.71825644e-02,\n",
       "        -4.90032703e-01, -6.00311875e-01, -7.24596262e-01, -3.92161250e-01,\n",
       "         4.89646457e-02,  8.71739462e-02,  5.20146042e-02,  6.51921153e-01,\n",
       "        -5.96600115e-01,  3.32118779e-01, -5.99505566e-02, -3.38748485e-01,\n",
       "        -4.33328003e-02, -5.05821645e-01, -4.28545952e-01,  7.56591976e-01,\n",
       "        -2.95098335e-01, -5.75058162e-02,  3.80677640e-01,  1.30226836e-01,\n",
       "         8.32868591e-02, -3.80796492e-01, -3.14125359e-01, -1.22307837e-02,\n",
       "         2.11083665e-01, -7.53503501e-01, -1.15129724e-01, -4.15399611e-01,\n",
       "        -8.03503811e-01, -6.25549078e-01, -7.01438934e-02,  9.60058868e-02,\n",
       "         3.95496413e-02, -2.26900309e-01, -2.05603138e-01, -1.40963286e-01,\n",
       "        -9.07166779e-01, -4.25353408e-01, -4.95373383e-02, -3.58888149e-01,\n",
       "        -2.17733696e-01,  2.09273607e-01, -5.51277757e-01, -2.13925958e-01,\n",
       "        -1.54050335e-01, -6.14240617e-02,  4.22918648e-01,  7.59871364e-01],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x), x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer(remove_stopwords=False, use_stems=False,\n",
    "                    to_lowercase=True, use_lemma=True, remove_punctuation=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_kwargs = dict(remove_stopwords=True, use_stems=False, to_lowercase=True, use_lemma=False, remove_punctuation=False)\n",
    "wv = create_word_vectorizer(tokenizer_kwargs=tok_kwargs, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-79c274859ac5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_to_word_vecs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtok_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "len(set(text_to_word_vecs(text, **tok_kwargs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_corpus = wv.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['!',\n",
       "  ',',\n",
       "  '.',\n",
       "  'bat',\n",
       "  'bats',\n",
       "  'door',\n",
       "  'echolocation',\n",
       "  'elephant',\n",
       "  'opened',\n",
       "  'potatoes',\n",
       "  'see',\n",
       "  'sight',\n",
       "  'sneeze',\n",
       "  'sneezed',\n",
       "  'studio',\n",
       "  'via',\n",
       "  'wondering'],\n",
       " {'elephant': 7,\n",
       "  'sneezed': 13,\n",
       "  'sight': 11,\n",
       "  'potatoes': 9,\n",
       "  '.': 2,\n",
       "  'bats': 4,\n",
       "  'see': 10,\n",
       "  'via': 15,\n",
       "  'echolocation': 6,\n",
       "  'bat': 3,\n",
       "  'sneeze': 12,\n",
       "  '!': 0,\n",
       "  'wondering': 16,\n",
       "  ',': 1,\n",
       "  'opened': 8,\n",
       "  'door': 5,\n",
       "  'studio': 14})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.get_feature_names(), wv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0],\n",
       "       [1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 2, 1, 1, 0, 0, 1, 0],\n",
       "       [0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_corpus.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " ',',\n",
       " '.',\n",
       " 'bat',\n",
       " 'bats',\n",
       " 'door',\n",
       " 'echolocation',\n",
       " 'elephant',\n",
       " 'opened',\n",
       " 'potatoes',\n",
       " 'see',\n",
       " 'sight',\n",
       " 'sneeze',\n",
       " 'sneezed',\n",
       " 'studio',\n",
       " 'via',\n",
       " 'wondering']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[0] for x in sorted(wv.vocabulary_.items(), key=lambda x: x[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [clean_text(t) for t in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = WordToVec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "class WordToVec(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, model:str='en_core_web_sm', vectorizer_kwargs=None, tokenizer_kwargs=None):\n",
    "        \n",
    "        self.nlp = spacy.load(model)\n",
    "        self.vectorizer = create_word_vectorizer(tokenizer_kwargs=tokenizer_kwargs, **vectorizer_kwargs)\n",
    "        self.tokenizer = Tokenizer(**tokenizer_kwargs)\n",
    "        self.last_transformed = []\n",
    "        \n",
    "    @property\n",
    "    def vocabulary(self):\n",
    "        return self.vectorizer.vocabulary_\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Learn how to transform data based on input data, X.\n",
    "        \"\"\"\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Transform X into a new dataset, Xprime and return it.\n",
    "        \"\"\"\n",
    "\n",
    "        for sent in X:\n",
    "            for word in sent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-00280e0695b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    460\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/github/nlpkungfu/nlpkf/vectorizers.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mtransf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtransf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_transformed_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvecs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array(list((w2v.fit_transform(res)))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'tolist'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6682783ec4de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'tolist'"
     ]
    }
   ],
   "source": [
    "np.array(w2v.fit_transform(res).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = w2v.fit_transform(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5, 384), (10, 384))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].shape, x[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CountVectorizer' object has no attribute 'vocabulary_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f793c5a22502>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'CountVectorizer' object has no attribute 'vocabulary_'"
     ]
    }
   ],
   "source": [
    "wv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tok_sp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-49f79136e2ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtok_corp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtok_sp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tok_sp' is not defined"
     ]
    }
   ],
   "source": [
    "tok_corp = tok_sp.fit_transform(corpus, return_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tok_corp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-f8354b6dfc5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtok_corp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tok_corp' is not defined"
     ]
    }
   ],
   "source": [
    "tok_corp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsv = GensimVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0.], dtype=float32),\n",
       " array([1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
       "        0., 0.], dtype=float32),\n",
       " array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
       "        1., 1.], dtype=float32)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(gsv.fit_transform(tok_corp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = tok_sp.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The elephant sneezed sight potato .',\n",
       " 'Bats see via echolocation . See bat sight sneeze !',\n",
       " 'Wondering , opened door studio .']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\" \".join([x for x in sent])  for sent in res ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_sp.use_stems = False\n",
    "tok_sp.use_lemma = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Argument 'string' has incorrect type (expected str, got list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-cd94805e2701>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable)\u001b[0m\n\u001b[1;32m    338\u001b[0m             raise ValueError(Errors.E088.format(length=len(text),\n\u001b[1;32m    339\u001b[0m                                                 max_length=self.max_length))\n\u001b[0;32m--> 340\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36mmake_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgolds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Argument 'string' has incorrect type (expected str, got list)"
     ]
    }
   ],
   "source": [
    "nlp(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Binarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = \n",
    "vectors = vectorizer.fit_transform([\" \".join(a) for a in tok_nltk.fit_transform(corpus)])\n",
    "vectors = vectorizer.fit_transform(corpus)\n",
    "freq   = CountVectorizer(tokenizer=tok_sp.tokenize_text)\n",
    "fit_corpus = freq.fit_transform(corpus)\n",
    "\n",
    "onehot = Binarizer()\n",
    "sk_corpus = onehot.fit_transform(fit_corpus.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>!</th>\n",
       "      <th>,</th>\n",
       "      <th>.</th>\n",
       "      <th>bat</th>\n",
       "      <th>door</th>\n",
       "      <th>echolocation</th>\n",
       "      <th>elephant</th>\n",
       "      <th>opened</th>\n",
       "      <th>potato</th>\n",
       "      <th>see</th>\n",
       "      <th>sight</th>\n",
       "      <th>sneeze</th>\n",
       "      <th>sneezed</th>\n",
       "      <th>studio</th>\n",
       "      <th>via</th>\n",
       "      <th>wondering</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   !  ,  .  bat  door  echolocation  elephant  opened  potato  see  sight  \\\n",
       "0  0  0  1    0     0             0         1       0       1    0      1   \n",
       "1  1  0  1    2     0             1         0       0       0    2      1   \n",
       "2  0  1  1    0     1             0         0       1       0    0      0   \n",
       "\n",
       "   sneeze  sneezed  studio  via  wondering  \n",
       "0       0        1       0    0          0  \n",
       "1       1        0       0    1          0  \n",
       "2       0        0       1    0          1  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(columns=freq.get_feature_names(), data=fit_corpus.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>!</th>\n",
       "      <th>,</th>\n",
       "      <th>.</th>\n",
       "      <th>bat</th>\n",
       "      <th>door</th>\n",
       "      <th>echolocation</th>\n",
       "      <th>elephant</th>\n",
       "      <th>opened</th>\n",
       "      <th>potato</th>\n",
       "      <th>see</th>\n",
       "      <th>sight</th>\n",
       "      <th>sneeze</th>\n",
       "      <th>sneezed</th>\n",
       "      <th>studio</th>\n",
       "      <th>via</th>\n",
       "      <th>wondering</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   !  ,  .  bat  door  echolocation  elephant  opened  potato  see  sight  \\\n",
       "0  0  0  1    0     0             0         1       0       1    0      1   \n",
       "1  1  0  1    2     0             1         0       0       0    2      1   \n",
       "2  0  1  1    0     1             0         0       1       0    0      0   \n",
       "\n",
       "   sneeze  sneezed  studio  via  wondering  \n",
       "0       0        1       0    0          0  \n",
       "1       1        0       0    1          0  \n",
       "2       0        0       1    0          1  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(columns=freq.get_feature_names(), data=fit_corpus.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'elephant': 6,\n",
       " 'sneezed': 12,\n",
       " 'sight': 10,\n",
       " 'potato': 8,\n",
       " '.': 2,\n",
       " 'bat': 3,\n",
       " 'see': 9,\n",
       " 'via': 14,\n",
       " 'echolocation': 5,\n",
       " 'sneeze': 11,\n",
       " '!': 0,\n",
       " 'wondering': 15,\n",
       " ',': 1,\n",
       " 'opened': 7,\n",
       " 'door': 4,\n",
       " 'studio': 13}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0],\n",
       "        [1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0],\n",
       "        [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1]], dtype=int64),\n",
       " array([[0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0],\n",
       "        [1, 0, 1, 2, 0, 1, 0, 0, 0, 2, 1, 1, 0, 0, 1, 0],\n",
       "        [0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1]], dtype=int64))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_corpus, fit_corpus.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['elephant sneezed sight potatoes .',\n",
       " 'bats see via echolocation . see bat sight sneeze !',\n",
       " 'wondering , opened door studio .']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\" \".join(a) for a in tok_nltk.fit_transform(corpus)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0],\n",
       "       [1, 1, 0, 1, 0, 0, 0, 2, 1, 1, 0, 0, 1, 0],\n",
       "       [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "gs_corpus  = tok_nltk.fit_transform(corpus)\n",
    "id2word = gensim.corpora.Dictionary(gs_corpus)\n",
    "vectors = [id2word.doc2bow(doc) for doc in gs_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-6bbf0cf3b383>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'doc' is not defined"
     ]
    }
   ],
   "source": [
    "[sent.vec for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)],\n",
       " [(0, 1), (3, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1)],\n",
       " [(0, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1)]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    [(token[0], 1) for token in id2word.doc2bow(doc)]\n",
    "    for doc in gs_corpus\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check 8104846059040039827 False\n",
      "out 1696981056005371314 False\n",
      "https://spacy.io 17142293684782158888 True\n",
      "(3, 3)\n",
      "3 3\n",
      "[0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.vocab import Vocab\n",
    "from spacy.attrs import ORTH, LIKE_URL, NORM\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u\"Check out https://spacy.io\")\n",
    "for token in doc:\n",
    "    print(token.text, token.orth, token.like_url)\n",
    "\n",
    "attr_ids = [ORTH, LIKE_URL, NORM]\n",
    "doc_array = doc.to_array(attr_ids)\n",
    "print(doc_array.shape)\n",
    "print(len(doc), len(attr_ids))\n",
    "\n",
    "assert doc[0].orth == doc_array[0, 0]\n",
    "assert doc[1].orth == doc_array[1, 0]\n",
    "assert doc[0].like_url == doc_array[0, 1]\n",
    "\n",
    "assert list(doc_array[:, 1]) == [t.like_url for t in doc]\n",
    "print(list(doc_array[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = Vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "lex = voc[\"Apple\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lex.norm_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EMBEDDINGS_LEN= 300\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'wv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-8eb4f1b7ef07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"EMBEDDINGS_LEN=\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEMBEDDINGS_LEN\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 300\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0membeddings_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEMBEDDINGS_LEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword2idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wv' is not defined"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "nlp = spacy.load('en_core_web_md')\n",
    " \n",
    "EMBEDDINGS_LEN = len(nlp.vocab['apple'].vector)\n",
    "print(\"EMBEDDINGS_LEN=\", EMBEDDINGS_LEN)  # 300\n",
    " \n",
    "embeddings_index = np.zeros((len(wv.get_feature_names()) + 1, EMBEDDINGS_LEN))\n",
    "for word, idx in word2idx.items():\n",
    "    try:\n",
    "        embedding = nlp.vocab[word].vector\n",
    "        embeddings_index[idx] = embedding\n",
    "    except:\n",
    "        pass\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  'The'),\n",
       " ('<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  'The',\n",
       "  'elephant'),\n",
       " ('<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  'The',\n",
       "  'elephant',\n",
       "  'sneezed'),\n",
       " ('<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  'The',\n",
       "  'elephant',\n",
       "  'sneezed',\n",
       "  'at'),\n",
       " ('<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  'The',\n",
       "  'elephant',\n",
       "  'sneezed',\n",
       "  'at',\n",
       "  'the'),\n",
       " ('<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  'The',\n",
       "  'elephant',\n",
       "  'sneezed',\n",
       "  'at',\n",
       "  'the',\n",
       "  'sight'),\n",
       " ('<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  'The',\n",
       "  'elephant',\n",
       "  'sneezed',\n",
       "  'at',\n",
       "  'the',\n",
       "  'sight',\n",
       "  'of'),\n",
       " ('<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  'The',\n",
       "  'elephant',\n",
       "  'sneezed',\n",
       "  'at',\n",
       "  'the',\n",
       "  'sight',\n",
       "  'of',\n",
       "  'potatoes.'),\n",
       " ('<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  'The',\n",
       "  'elephant',\n",
       "  'sneezed',\n",
       "  'at',\n",
       "  'the',\n",
       "  'sight',\n",
       "  'of',\n",
       "  'potatoes.',\n",
       "  ''),\n",
       " ('<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  'The',\n",
       "  'elephant',\n",
       "  'sneezed',\n",
       "  'at',\n",
       "  'the',\n",
       "  'sight',\n",
       "  'of',\n",
       "  'potatoes.',\n",
       "  '',\n",
       "  '<$>'),\n",
       " ('<#>',\n",
       "  '<#>',\n",
       "  '<#>',\n",
       "  'The',\n",
       "  'elephant',\n",
       "  'sneezed',\n",
       "  'at',\n",
       "  'the',\n",
       "  'sight',\n",
       "  'of',\n",
       "  'potatoes.',\n",
       "  '',\n",
       "  '<$>',\n",
       "  '<$>'),\n",
       " ('<#>',\n",
       "  '<#>',\n",
       "  'The',\n",
       "  'elephant',\n",
       "  'sneezed',\n",
       "  'at',\n",
       "  'the',\n",
       "  'sight',\n",
       "  'of',\n",
       "  'potatoes.',\n",
       "  '',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>'),\n",
       " ('<#>',\n",
       "  'The',\n",
       "  'elephant',\n",
       "  'sneezed',\n",
       "  'at',\n",
       "  'the',\n",
       "  'sight',\n",
       "  'of',\n",
       "  'potatoes.',\n",
       "  '',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>'),\n",
       " ('The',\n",
       "  'elephant',\n",
       "  'sneezed',\n",
       "  'at',\n",
       "  'the',\n",
       "  'sight',\n",
       "  'of',\n",
       "  'potatoes.',\n",
       "  '',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>'),\n",
       " ('elephant',\n",
       "  'sneezed',\n",
       "  'at',\n",
       "  'the',\n",
       "  'sight',\n",
       "  'of',\n",
       "  'potatoes.',\n",
       "  '',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>'),\n",
       " ('sneezed',\n",
       "  'at',\n",
       "  'the',\n",
       "  'sight',\n",
       "  'of',\n",
       "  'potatoes.',\n",
       "  '',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>'),\n",
       " ('at',\n",
       "  'the',\n",
       "  'sight',\n",
       "  'of',\n",
       "  'potatoes.',\n",
       "  '',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>'),\n",
       " ('the',\n",
       "  'sight',\n",
       "  'of',\n",
       "  'potatoes.',\n",
       "  '',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>'),\n",
       " ('sight',\n",
       "  'of',\n",
       "  'potatoes.',\n",
       "  '',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>'),\n",
       " ('of',\n",
       "  'potatoes.',\n",
       "  '',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>'),\n",
       " ('potatoes.',\n",
       "  '',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>'),\n",
       " ('',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>',\n",
       "  '<$>')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals = [list(nltk.ngrams(doc.split(\" \"), 14, pad_left=True, pad_right=True, left_pad_symbol='<#>', right_pad_symbol='<$>')) for doc in corpus]\n",
    "vals[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.ngrams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-080d925ba1a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# vocab_size is the number of words in your train, val and test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# vector_size is the dimension of the word vectors you are using\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# intialize the word vectors, pretrained_weights is a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocab_size' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "# vocab_size is the number of words in your train, val and test set\n",
    "# vector_size is the dimension of the word vectors you are using\n",
    "embed = nn.Embedding(vocab_size, vector_size)\n",
    "\n",
    "# intialize the word vectors, pretrained_weights is a \n",
    "# numpy array of size (vocab_size, vector_size) and \n",
    "# pretrained_weights[i] retrieves the word vector of\n",
    "# i-th word in the vocabulary\n",
    "embed.weight.data.copy_(torch.fromnumpy(pretrained_weights))\n",
    "\n",
    "# Then turn the word index into actual word vector\n",
    "vocab = {\"some\": 0, \"words\": 1}\n",
    "word_indexes = [vocab[w] for w in [\"some\", \"words\"]] \n",
    "word_vectors = embed(word_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
